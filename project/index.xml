<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Projects | Jeremy Yeaton</title>
    <link>https://jeremyyeaton.github.io/project/</link>
      <atom:link href="https://jeremyyeaton.github.io/project/index.xml" rel="self" type="application/rss+xml" />
    <description>Projects</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Fri, 01 Jan 2021 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://jeremyyeaton.github.io/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_3.png</url>
      <title>Projects</title>
      <link>https://jeremyyeaton.github.io/project/</link>
    </image>
    
    <item>
      <title>Letter position coding</title>
      <link>https://jeremyyeaton.github.io/project/letter-perception/</link>
      <pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://jeremyyeaton.github.io/project/letter-perception/</guid>
      <description>&lt;p&gt;Misspelled words containing repeated letters that already occur in the word like &lt;em&gt;repetitition&lt;/em&gt; are much harder to identify as misspelled than if letters were added that weren&amp;rsquo;t already in the word like &lt;em&gt;repetiglition&lt;/em&gt;. The same is true if you swap two letters in a word vs. replace one of them with another letter. There are a number of ways that our brain can trick itself into thinking a non-word is a word, but all of these come down to the question &lt;em&gt;how does the brain know where letters are in a word?&lt;/em&gt; Are the letters coded in slots like c1 a2 t3? Or maybe they&amp;rsquo;re coded as being relative to one another &amp;ndash; c before a, a before t.&lt;/p&gt;
&lt;p&gt;In a series of online letter identification experiments manipulating where the word appears on the screen, and when and how we tell the participants what their target is (before or after presentation), we are moving to better understand how the brain codes letter position information in the visual field and within a word.&lt;/p&gt;
&lt;p&gt;This experiment is part of a larger ERC Advanced Grant looking at parallel orthographic and sentence processing during reading.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Statistical learning in noise</title>
      <link>https://jeremyyeaton.github.io/project/sequence-learning/</link>
      <pubDate>Tue, 01 Sep 2020 00:00:00 +0000</pubDate>
      <guid>https://jeremyyeaton.github.io/project/sequence-learning/</guid>
      <description>&lt;p&gt;In a series of experiments using a motor sequence learning task, we are examining the effect of different types of noise on the baboons&#39; ability to extract a regularity (e.g.: AB) from a sequence. For example, if the baboon saw the pair AB 100 times, but each time it was in a different position in the sequence (e.g.: XABX, XXAB, ABXX, etc.), will that impede their ability to learn the relationship between A and B relative to if they only saw ABCD every trial?&lt;/p&gt;
&lt;p&gt;In a parallel project, we are constructing a computational model of this learning process to better understand how the process takes place in baboons, and how that differs from in humans.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Parallel processing in reading</title>
      <link>https://jeremyyeaton.github.io/project/pop-r/</link>
      <pubDate>Sun, 01 Sep 2019 00:00:00 +0000</pubDate>
      <guid>https://jeremyyeaton.github.io/project/pop-r/</guid>
      <description>&lt;p&gt;This project is the umbrella for a large research program encompassing behavioral, eye-tracking, modelling, and neuroimaging experiments as part of an ERC Advanced Grant looking at parallel orthographic and sentence processing during reading. The PI on this project is Jonathan Grainger.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Ambiguity in negative dependencies</title>
      <link>https://jeremyyeaton.github.io/project/negation/</link>
      <pubDate>Tue, 01 Sep 2015 00:00:00 +0000</pubDate>
      <guid>https://jeremyyeaton.github.io/project/negation/</guid>
      <description>&lt;p&gt;Many languages exhibit Negative Concord, that is when two negative items are interpreted as a single negation. For example, if a man has robbed a bank and says &lt;em&gt;I didn&amp;rsquo;t do nothing&lt;/em&gt;, we understand this as &lt;em&gt;I didn&amp;rsquo;t do anything&lt;/em&gt; even though he used two negative words&amp;ndash;&lt;em&gt;didn&amp;rsquo;t&lt;/em&gt; and &lt;em&gt;nothing&lt;/em&gt;. By contrast, if a mother comes home from work and finds her child sitting in front of the television, she might accuse them of having done nothing all day, to which they might reply &lt;em&gt;I didn&amp;rsquo;t do nothing&lt;/em&gt;. The child, however means exactly the opposite of what the bank robber meant &amp;ndash; that they &lt;em&gt;did&lt;/em&gt; do &lt;em&gt;something&lt;/em&gt;. French speakers have been shown to easily access both of these readings (Déprez et al. 2015), so we&amp;rsquo;ve undertaken a series of experiments to see how context (Déprez &amp;amp; Yeaton 2018), and prosody (Yeaton &amp;amp; Déprez forthcoming) might be employed to convey meaning. We also want to examine how good listeners are at leveraging these prosodic signals to access the meaning that the speaker intended (Loder et al. forthcoming).&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
